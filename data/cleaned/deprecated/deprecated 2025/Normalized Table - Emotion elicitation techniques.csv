paper_id,model_id,doi,apa_citation,is_multimodal,modality_visual,modality_auditory,modality_somatosensory,task_type_active,task_type_passive,technique_name,visual_pictures,visual_videos,visual_words,visual_other,auditory_music,auditory_other,technique_clasif_multiple_techniques,technique_clasif_driving,technique_clasif_imagination_techniques_or_memory_recall,technique_clasif_social_interactions,technique_clasif_virtual_reality,technique_clasif_meditation,technique_clasif_reading,technique_clasif_ux,technique_clasif_tactile_enhanced_multimedia_clips,technique_clasif_videogame,technique_clasif_puzzle,technique_clasif_other,technique_description,elicitation_duration,elicitation_duration_range,elicitation_duration_mean,elicitation_duration_median,revisor,comentarios,Unnamed: 36,Unnamed: 37
1,1,,"Zangróniz, R., Martínez-Rodrigo, A., Pastor, J. M., López, M. T., & Fernández-Caballero, A. (2017). Electrodermal Activity Sensor for Classification of Calm/Distress Condition. Sensors (Basel, Switzerland), 17(10), E2324. https://doi.org/10.3390/s17102324
",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,,"The participant sits in front of the experimentation monitor and the wearable is put in the wrist of the non-dominant hand (see Figure 1). In this regard, the experimentation monitor consists of a high resolution, 28 inch screen. When the technician verifies the proper functioning of the wearable and its communication with the software, the experiment starts. Firstly, the participant has to carefully read the general instructions of the experiment. Then, ten pictures that are labelled with high arousal and low valence are shown consecutively during 6 s each to the participant. Silences consisting of blank images with a fixed duration of 1 s are inserted before each picture used from the database. The pictures are selected randomly from the set of images that fulfil the condition. Therefore, the segment used for subsequent analysis is 70 s long (10 pictures 6 s duration, plus one blank image before each picture). In this sense, a single presentation of many stimuli presented for a short period of time might favour the continuity of emotional state [29]. Afterwards, a distracting task is presented to the participant so that his/her emotional state comes to neutral. Next, the experiment continues by showing randomly another set of ten images from IAPS that fulfil the condition to be part of those previously labelled as low arousal and high valence. Therefore, two segments of data from each individual are finally obtained, one for calm condition and another for distress condition. Again, silences are used before each picture. Lastly, the distracting task is offered again.",6s,-,-,-,,,,
2,1,,"Liu, M., Fan, D., Zhang, X., & Gong, X. (2017). Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM. 157–160. Scopus. https://doi.org/10.1109/ICSCSE.2016.0051
",x,x,x,-,-,x,-,?,,,,?,?,-,,-,-,-,,,,,,,,"All subjects are Junior in Jilin University of
Changchun, China. Materials for eliciting emotions are 4
video fragments cut elaborately from large amounts of
movies representing 4 different emotions, such as happiness,
grief, anger, and fear. Each fragment last 4-5 minutes and
was connected with a short video which also last 4-5
minutes for calm recovery. And we add the emotion when
people stay in calm. So there are 5 emotions that make the
pattern recognition.",-,4-5 m,-,-,,,,
3,1,,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
4,1,,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061905",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
5,1,,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,,-,6s,-,-,-,,,,
6,1,,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",x,x,x,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,,-,-,3m-5m,-,-,,,,
7,1,,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",x,x,x,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,,,-,-,-,-,,,,
8,1,,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-,,,,
9,1,,"Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S. (2018). Electrodermal Activity based Classification of Induced Stress in a Controlled Setting. MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings. Scopus. https://doi.org/10.1109/MeMeA.2018.8438703",x,-,-,,x,-,Trier Social Stress Test (TSST),-,,,-,-,-,x,,-,-,-,-,,,,,,,"A participant is given 3-5 minutes to prepare for a 5-minute presentation, followed by a 5-minute mental arithmetic task of counting numbers backwards continuously in large steps. The unanticipated element is that a large audience enters the room after the preparatory phase trying to induce stress in the form of stage fright. In the end, the participant is briefed and made to relax.",15m,-,-,-,,,,
10,1,,"Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K. (2018). Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living. 222–228. Scopus. https://doi.org/10.1145/3197768.3201523",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
11,1,,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589",x,x,x,-,-,x,-,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,1,,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",x,x,x,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-,,,,
13,1,,"Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S. (2018). Galvanic skin response data classification for emotion detection. International Journal of Electrical and Computer Engineering, 8(5), 4004–4014. Scopus. https://doi.org/10.11591/ijece.v8i5.pp4004-4014",x,x,x,-,x,-,-,-,,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
14,1,,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,,,,
14,2,,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,,,,
15,1,,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,x,x,-,-,x,-,x,x,,-,-,-,x,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
15,2,,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
15,3,,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
15,4,,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,x,x,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
15,5,,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,x,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,,-,-,-,-,-,,,,
16,1,,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
17,1,,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,x,x,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,,,,
18,1,,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533",x,x,x,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,,15m,,,,,
19,1,,"Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F. (2017). Emotion and attention: Predicting electrodermal activity through video visual descriptors. 914–923. Scopus. https://doi.org/10.1145/3106426.3109418",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,"In order to collect ground truth data for the present study, the
EDA was measured on 22 subjects, 10 men and 12 women, with ages
between 21 and 59 years old, while they watched a concantenation
of videos. The data set consists of 44 videos with an average duration
of 44 seconds (σ = 21) which belong to the awarded spots at the 2002
Cannes International Advertising Festival. The whole sequence of
spots was projected in a movie theater while the EDA was recorded
on each subject by means of the Sociograph device",-,-,44s,-,,,,
20,1,,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
21,1,,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,,,,
22,1,,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",x,x,x,-,-,x,,-,,,,,,,,,,,,,,,,,,,,,,,,,,
22,2,,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,5s,-,-,-,,,,
22,3,,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,1m-2-m,-,-,,,,
23,1,,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
24,1,,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677",-,-,x,-,-,x,IADS,-,-,,-,-,-,-,-,-,-,-,-,,,,,,,"During the
experiment, participants were seated in a comfortable chair
in a controlled environment while listening to the IADS
sounds. Each subject was left alone in the room where the
experiment took place for the whole duration (29 minutes).
The acoustic stimulation was performed by using headphones
while the subject’s eyes were closed, to avoid any kind of
visual interference.
The experimental protocol consisted of 8 sessions: after
an initial resting session of 5 minutes, three arousal sessions
alternated with neutral sessions (see Figure 2). Each arousal
and neutral session was different from the others in regard
to the arousal level (labeled as N (neutral), L (low), M
(medium) and H (high)). We selected four arousal ranges that
were not overlapped. Such levels were set according to the
IADS scores reported in table I. Within each arousing session,
the acoustic stimuli were selected to have both negative and
positive valence. Each neutral session lasted 1 minute and 28
seconds, while the three arousal sessions had a duration of
3 minutes and 40 seconds, 4 minutes, and 5 minutes and 20
seconds, respectively. The different duration of each arousal
session is due to the different length of acoustic stimuli having
the same range of positive and negative valence.",-,1 m 28 s - 5 m 20 s,-,-,,,,
25,1,,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,x,-,-,-,x,,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,each picture display contained five pictures and was displayed for 4 seconds each,4s,-,-,-,,,,
25,2,,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,x,x,-,-,x,,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,,4s,-,-,-,,,,
25,3,,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,-,x,-,-,x,,-,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
26,1,,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488–492. Scopus. https://doi.org/10.1109/IHMSC.2016.66",-,,,,x,-,-,,,,,,,,,,,,,,,,,,,"In the experiment, the process of collecting
the signal of the emotions was playing neutral materials
which were to keep mood in peace for 2 minutes,
introducing the materials background and the target
emotion for 30 seconds, and playing the materials of the
target emotion for 5 minutes. If the subject felt that a
corresponding emotion was generated, the button next to
the seat should be pressed to make a mark. Finally, after
collecting the signal of target emotion, the subjects
needed to evaluate the effectiveness of emotion had been
induced, which the type of the emotion had been induced
and intensity of emotion (very strong, strong, weak, very
weak).",,,,,,,,
27,1,,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-,,,,
28,1,,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,-,2m34s,-,-,-,,,,
29,1,,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,,,,
30,1,,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",x,x,x,-,-,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-,,,,
31,1,,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,,-,15m,-,-,-,,,,
32,1,,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
33,1,,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,,-,15m,-,-,-,,,,
34,1,,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516647",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
35,1,,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
36,1,,"Zhang, S., Liu, G., & Lai, X. (2015). Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals. Journal of Advanced Computational Intelligence and Intelligent Informatics, 19(1), 118-126.",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
37,1,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,x,x,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,-,51s –127 s,-,-,,,,
37,2,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
37,3,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,x,x,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,,-,30m,-,-,-,,,,
37,4,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
37,5,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
37,6,,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
38,1,,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
39,1,,"Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V. (2017). Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture. Pervasive and Mobile Computing, 34, 46–59. Scopus. https://doi.org/10.1016/j.pmcj.2016.04.006",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,9m,-,-,-,,,,
40,1,,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,-,-,,,,
41,1,,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",x,x,x,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,,15m,,,,,
42,1,,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,,884s,,,,,
43,1,,"Barral, O., Kosunen, I., & Jacucci, G. (2017). No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment. ACM Transactions on Computer-Human Interaction, 24(6). Scopus. https://doi.org/10.1145/3157730",-,x,-,-,x,-,-,-,-,,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,,
44,1,,"Lanatà, A., Valenza, G., & Scilingo, E. P. (2012). A novel EDA glove based on textile-integrated electrodes for affective computing. Medical & Biological Engineering & Computing, 50(11), 1163–1172. doi:10.1007/s11517-012-0921-9",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,-,-,-,-,-,-,,,,,,-,10 s,-,-,-,,,,
45,1,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",-,-,-,-,x,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,,-,3s,-,-,-,,,,
46,1,,"GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. (2011). Emotion recognition from physiological signals. Journal of Medical Engineering & Technology, 35(6-7), 300–307. doi:10.3109/03091902.2011.601784",-,x,-,-,-,-,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
47,1,,"Bornoiu, I.-V., Strungaru, R., & Grigore, O. (2015). Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing. 6th European Conference of the International Federation for Medical and Biological Engineering, 70–73. doi:10.1007/978-3-319-11128-5_18 ",-,x,x,-,x,-,Trier social stress test,x,,,-,x,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
48,1,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",-,-,-,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
49,1,,"Drungilas, D., Bielskis, A. A., & Denisov, V. (2010). An intelligent control system based on non-invasive man machine interaction. In Innovations in Computing Sciences and Software Engineering (pp. 63-68). Springer, Dordrecht.",-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,1,,"Wu, G., Liu, G., & Hao, M. (2010). The Analysis of Emotion Recognition from GSR Based on PSO. 2010 International Symposium on Intelligence Information Processing and Trusted Computing. doi:10.1109/iptc.2010.60",-,x,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,240-300s,-,-,,,,
51,1,,"Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G. (2011). Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features. IEEE Transactions on Affective Computing, 2(3), 119–133. doi:10.1109/t-affc.2011.4 ",-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,-,-,-,-,,,,
52,1,,"Safta, I., Grigore, O., & Căruntu, C.(2011). Emotion Detection Using Psycho-Physiological Signal Processing. Computer, 3, 4.",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,,-,7s,-,-,-,,,,
53,1,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",-,x,-,-,x,-,Stroop,-,,x,-,-,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
54,1,,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",-,x,x,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
55,1,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",-,-,-,-,x,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,,-,-,-,-,-,,,,
56,1,,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.",-,x,x,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,,-,-,300-480s,-,-,,,,
57,1,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",-,-,-,-,x,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,,robot interaction,-,10-30s,-,-,,,,
58,1,,"Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G. (2014). MoodMagician. Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems - SenSys ’14. doi:10.1145/2668332.2668371",-,x,x,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,,-,-,240s-300s,-,-,,,,
59,1,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
60,1,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
60,2,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
61,1,,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,00 m 07 ss,,,,,,,
62,1,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",x,x,x,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,,,,
63,1,,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
64,1,,"Dar, M. N., Akram, M. U., Khawaja, S. G., & Pujari, A. N. (2020). Cnn and lstm-based emotion charting using physiological signals. Sensors, 20(16), 4551.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
65,1,,"Greco, A., Marzi, C., Lanata, A., Scilingo, E. P., & Vanello, N. (2019, July). Combining electrodermal activity and speech analysis towards a more accurate emotion recognition system. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 229-232). IEEE.",,x,,,,x,,,,x,,,,,,,,,,,,,,,,,0m 2s,,,,,,,
66,1,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
67,1,,"Lee, S., Lee, T., Yang, T., Yoon, C., & Kim, S. P. (2020). Detection of drivers’ anxiety invoked by driving situations using multimodal biosignals. Processes, 8(2), 155.",x,x,x,,,x,,,x,,,,,,,,,,,,,,,,,,0m 30s,,,,,,,
68,1,,"García-Faura, Á., Hernández-García, A., Fernández-Martínez, F., Díaz-de-María, F., & San-Segundo, R. (2019, January). Emotion and attention: Audiovisual models for group-level skin response recognition in short movies. In Web Intelligence (Vol. 17, No. 1, pp. 29-40). IOS Press.",x,x,x,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,,,,
69,1,,"Wei, W., Jia, Q., Feng, Y., & Chen, G. (2018). Emotion recognition based on weighted fusion strategy of multichannel physiological signals. Computational intelligence and neuroscience, 2018.",x,x,x,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,,-,34.9s - 117 s,"81,4 s",-,,,,
70,1,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
71,1,,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
72,1,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,
73,1,,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,,00 m 120 s,,,,,
74,1,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
75,1,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,,,,
76,1,,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.",,,x,,,,,,,,,x,,,,,,,,,,,,,,,,00 m 10 s - 01m 30 s,,,,,,
77,1,,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,
78,1,,"Raheel, A., Majid, M., Alnowami, M., & Anwar, S. M. (2020). Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia. Sensors, 20(14), 4037.",,,,,,,,,,,,,,,,,,,,,,x,,,,,,00 m 21 s - 00 m 58 s,,,,,,
79,1,,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,
80,1,,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,,,,,,
81,1,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,x,x,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,
81,2,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,x,x,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,
82,1,,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
83,1,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,x,,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,
84,1,,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).",,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,
85,1,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,x,x,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,32-245s,68s,,,,,
86,1,,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
87,1,,"Yasemin, M., Sarıkaya, M. A., & Ince, G. (2019, July). Emotional state estimation using sensor fusion of EEG and EDA. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5609-5612). IEEE.",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,,"we conducted the experiments with
audiovisual stimuli to induce emotions by playing videos.
We have used a video prepared in [21], which contains
several kinds of movie clips and binaural beats 1. As stimuli,
we chose movies which have high effect to arouse human
emotions. A binaural beat is an auditory sensation, which
appears when two slightly different sounds are received to",-,-,-,-,,,,
88,1,,"Ghiasi, S., Greco, A., Barbieri, R., Scilingo, E. P., & Valenza, G. (2020). Assessing autonomic function from electrodermal activity and heart rate variability during cold-pressor test and emotional challenge. Scientific reports, 10(1), 1-13.",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,,-,1M 30S,-,-,-,,,,
89,1,,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,,-,0M 06S,-,-,-,,,,
90,1,,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,1,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,,,,,,
92,1,,"Rahman, J. S., Hossain, M. Z., & Gedeon, T. (2019, December). Measuring Observers' EDA Responses to Emotional Videos. In Proceedings of the 31st Australian Conference on Human-Computer-Interaction (pp. 457-461).",x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,,-,-,0M 2S - 0M3S,-,-,,,,
93,1,,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
94,1,,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,,,,,,
95,1,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,x,x,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,,00m 52 s - 23m 58s,,,,,,
96,1,,"Kołodziej, M., Tarnowski, P., Majkowski, A., & Rak, R. J. (2019). Electrodermal activity measurements for detection of emotional arousal. Bulletin of the Polish Academy of Sciences. Technical Sciences, 67(4).",x,x,x,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,,-,-,0M 4.9S - 0M 71.1S,-,-,,,,
97,1,,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.",x,x,x,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,1m,-,-,-,,,,
98,1,,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",x,x,x,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,,-,-,51s –127 s,-,-,,,,
99,1,,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).",,x,,,,,IAPS,x,,,,,,,,,,,,,,,,,,,00 m 06 ss,,,,,,,
100,1,10.1016/j.trf.2024.12.031,"Kim, T., Kim, S., Lee, M., Kang, Y., & Hwang, S. (2025). Assessing human emotional experience in pedestrian environments using wearable sensing and machine learning with anomaly detection. Transportation Research Part F: Traffic Psychology and Behaviour, 109, 540-555.",-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Real‑world urban walking,A walking path … 10–15 min walking path divided into eight distinct areas​,,10 m 00 s – 15 m 00 s ​,-,-,Tomi,,,
101,1,10.3390/s24248130,"Mercado-Diaz, L. R., Veeranki, Y. R., Large, E. W., & Posada-Quintero, H. F. (2024). Fractal Analysis of Electrodermal Activity for Emotion Recognition: A Novel Approach Using Detrended Fluctuation Analysis and Wavelet Entropy. Sensors, 24(24), 8130.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
102,1,10.1109/TVCG.2024.3372101,"Li, M., Pan, J., Li, Y., Gao, Y., Qin, H., & Shen, Y. (2024). Multimodal physiological analysis of impact of emotion on cognitive control in VR. IEEE Transactions on Visualization and Computer Graphics.",x,x,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Eight 360° VR clips inducing four arousal–valence states,,01 m 40 s – 05 m 50 s ​,210 s,-,Tomi,,,
103,1,10.1016/j.irbm.2024.100849,"Veeranki, Y. R., Posada-Quintero, H. F., & Swaminathan, R. (2024). Transition Network-Based Analysis of Electrodermal Activity Signals for Emotion Recognition. IRBM, 45(4), 100849.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
104,1,10.1109/JSEN.2024.3354553,"Veeranki, Y. R., Mercado Diaz, L. R., Swaminathan, R., & Posada-Quintero, H. F. (2024). Nonlinear Signal Processing Methods for Automatic Emotion Recognition Using Electrodermal Activity. IEEE Sensors Journal, 24(6), 3354553.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
105,1,10.1109/ACII55700.2022.9953862,"Surely, A., Taherzadeh, S., Misal, V., & Kleinsmith, A. (2022, October). Exploring Affective Dimension Perception from Bodily Expressions and Electrodermal Activity in Paramedic Simulation Training. In 2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-8). IEEE.",x,x,x,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,High-fidelity emergency simulation,Paramedic trainees manage realistic emergency on manikin inside mock home/ambulance,,10 m 00 s – 20 m 00 s ​,-,-,Tomi,,,
106,1,10.1038/s41597-024-03676-4,"Yang, P., Liu, N., Liu, X., Shu, Y., Ji, W., Ren, Z., ... & Liu, Y. J. (2024). A Multimodal Dataset for Mixed Emotion Recognition. Scientific Data, 11(1), 847.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,32 film clips were presented,The display of one video clip for about 20-30 seconds.,00 m 20 s – 00 m 30 s ​,-,-,Tomi,,,
107,1,10.1515/cdbme-2021-2220,"Rao Veeranki, Y., Ganapathy, N., & Swaminathan, R. (2021). Electrodermal activity based emotion recognition using time-frequency methods and machine learning algorithms. Current Directions in Biomedical Engineering, 7(2), 863-866.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
108,1,10.1109/ICCCMLA58983.2023.10346619,"Ronickom, J. F. A. (2023, October). Enhancing emotion recognition: Machine learning with phasic spectrogram texture features. In 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA) (pp. 600-603). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
109,1,10.1109/TAFFC.2023.3265433,"Bota, P., Zhang, T., El Ali, A., Fred, A., da Silva, H. P., & Cesar, P. (2023). Group synchrony for emotion recognition using physiological signals. IEEE Transactions on Affective Computing, 14(4), 2614-2625.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
109,2,10.1109/TAFFC.2023.3265433,"Bota, P., Zhang, T., El Ali, A., Fred, A., da Silva, H. P., & Cesar, P. (2023). Group synchrony for emotion recognition using physiological signals. IEEE Transactions on Affective Computing, 14(4), 2614-2625.",x,x,x,-,x,-,Semi-structured paired debate,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,"10-min English debate on admitting Yemeni refugees, with free turn-taking",10 m 00 s,-,-,-,Tomi,,,
110,1,10.1109/TIPTEKNO56568.2022.9960200,"Semerci, Y. C., Akgün, G., Toprak, E., & Barkana, D. E. (2022, October). A comparative analysis of deep learning methods for emotion recognition using physiological signals for robot-based intervention studies. In 2022 Medical Technologies Congress (TIPTEKNO) (pp. 1-4). IEEE.",-,x,-,-,-,x,Pictures,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Participants viewed a series of emotional pictures designed to elicit specific emotional states.,"The stimulus presentation for each picture was 6s, followed by a 6s inter-stimulus interval (ISI)",-,-,-,Tomi,,,
111,1,10.34107/YHPN9422.04322,"Veeranki, Y. R., Ganapathy, N., & Swaminathan, R. (2021). Differentiation of dichotomous emotional states in electrodermal activity signals using higher-order crossing features and parametric classifiers. Biomed. Sci. Instr, 57(2), 322-332.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
112,1,10.1149/10701.12535ecst,"Dutta, S., Mishra, B. K., Mitra, A., & Chakraborty, A. (2022). An analysis of emotion recognition based on GSR signal. ECS Transactions, 107(1), 12535.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 36 affective movie clips,-,51–127s,80,-,Tomi,,,
113,1,10.1007/s11277-023-10685-w,"Selvi, R., & Vijayakumaran, C. (2023). An efficient multimodal emotion identification using FOX optimized double deep Q-learning. Wireless Personal Communications, 132(4), 2387-2406.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 36 affective movie clips,-,51–127s,80,-,Tomi,,,
114,1,10.1088/1742-6596/1878/1/012020,"Bulagang, A. F., Mountstephens, J., & Teo, J. (2021, May). Support Vector Machine Tuning for Improving Four-Quadrant Emotion Prediction in Virtual Reality (VR) using Wearable Electrodermography (EDG). In Journal of Physics: Conference Series (Vol. 1878, No. 1, p. 012020). IOP Publishing.",x,x,x,-,-,x,Virtual Reality,-,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,,Virtual Reality (VR),A time span of 6 minutes and 5 seconds of the merged videos were presented to the participants.,-,320 s,-,Tomi,,,
115,1,10.1016/j.bspc.2024.107039,"Kumar, A., & Kumar, A. (2025). Human emotion recognition using Machine learning techniques based on the physiological signal. Biomedical Signal Processing and Control, 100, 107039.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
115,2,10.1016/j.bspc.2024.107039,"Kumar, A., & Kumar, A. (2025). Human emotion recognition using Machine learning techniques based on the physiological signal. Biomedical Signal Processing and Control, 100, 107039.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 36 affective movie clips,-,51–127s,80,-,Tomi,,,
116,1,10.1109/TAFFC.2019.2916015,"Jung, T. P., & Sejnowski, T. J. (2019). Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. IEEE Transactions on Affective Computing, 13(1), 96-107.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
116,2,10.1109/TAFFC.2019.2916015,"Jung, T. P., & Sejnowski, T. J. (2019). Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. IEEE Transactions on Affective Computing, 13(1), 96-107.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
116,3,10.1109/TAFFC.2019.2916015,"Jung, T. P., & Sejnowski, T. J. (2019). Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. IEEE Transactions on Affective Computing, 13(1), 96-107.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 20 short emotional film excerpts selected to elicit specific affective states​​,-,34.9-117 s​,81.4 s​​,-,Tomi,,,
117,1,10.1016/j.teler.2024.100131,"Saffaryazdi, N., Kirkcaldy, N., Lee, G., Loveys, K., Broadbent, E., & Billinghurst, M. (2024). Exploring the impact of computer-mediated emotional interactions on human facial and physiological responses. Telematics and Informatics Reports, 14, 100131.",x,x,-,-,x,,IAPS-guided autobiographical conversation,x,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,,"Viewing an IAPS picture (6 s) followed by 2–3 min guided discussion of personal memories/emotions linked to that picture, in either face-to-face or Zoom condition",The image remained on the screen during the conversation for two to three minutes.,02 m 09 s – 03 m 09 s,-,-,Tomi,,,
118,1,10.1109/IECBES48179.2021.9398844,"Nisa'Minhad, K., Ooi, K. J., Bhuiyan, M. A. S., Reaz, M. B. I., & Ali, S. H. M. (2021, March). Assessments of autonomic nervous system biomarker for emotion recognition using electrodermal activity signal. In 2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 351-355). IEEE.",x,x,x,-,-,x,Video Stimuli; Pictures,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,,-,-,-,Tomi,,,
119,1,10.1109/ACCESS.2024.3361832,"Veeranki, Y. R., Ganapathy, N., Swaminathan, R., & Posada-Quintero, H. F. (2024). Comparison of electrodermal activity signal decomposition techniques for emotion recognition. IEEE Access, 12, 19952-19966.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
120,1,10.1109/ACIIW52867.2021.9666360,"Bhatti, A., Behinaein, B., Rodenburg, D., Hungler, P., & Etemad, A. (2021, September). Attentive cross-modal connections for deep multimodal wearable-based emotion recognition. In 2021 9th international conference on affective computing and intelligent interaction workshops and demos (ACIIW) (pp. 01-05). IEEE.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
121,1,10.3389/fpsyg.2022.895929,"Chong, D., Yu, A., Su, H., & Zhou, Y. (2022). The impact of emotional states on construction workers’ recognition ability of safety hazards based on social cognitive neuroscience. Frontiers in psychology, 13, 895929.",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,"Thirty images each of positive, neutral and negative … selected from the IAPS",01 m 30 s,1,-,-,Tomi,,,
122,1,10.1145/3534615,"Gupta, K., Chan, S. W., Pai, Y. S., Strachan, N., Su, J., Sumich, A., ... & Billinghurst, M. (2022). Total vrecall: Using biosignals to recognize emotional autobiographical memory in virtual reality. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(2), 1-21.",-,x,-,-,x,,Modified Autobiographical Memory Test (AMT),-,-,x,-,-,-,-,-,-,-,x,-,-,-,-,-,-,,"Single emotional cue word (5 positive, 5 negative, 5 neutral) shown in VR; participant silently recalls a self-relevant memory for 30 s, then narrates it.",07 m 30 s,-,-,-,Tomi,,,
123,1,10.3233/shti240569,"Banik, S., Kumar, H., Ganapathy, N., & Swaminathan, R. (2024). Assessment of Valance Emotional State Using EEG-EDA Coupling and Explainable Classifiers. In Digital Health and Informatics Innovations for Sustainable Health Care Systems (pp. 953-957). IOS Press.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
124,1,10.3389/fnins.2023.1180407,"Wang, K., Zhao, Z., Shen, X., & Yamauchi, T. (2023). Video elicited physiological signal dataset considering indoor temperature factors. Frontiers in Neuroscience, 17, 1180407.",x,x,x,x,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,"Participants watched 25 one-minute video clips (5 per emotion) in hot, comfortable, and cold rooms while GSR was recorded. (drawn from Table 1 & § 3.4.3)",06 m 00 s,-,-,-,Tomi,,,
125,1,10.1109/JBHI.2022.3225330,"Zitouni, M. S., Park, C. Y., Lee, U., Hadjileontiadis, L. J., & Khandoker, A. (2022). LSTM-modeling of emotion recognition using peripheral physiological signals in naturalistic conversations. IEEE Journal of Biomedical and Health Informatics, 27(2), 912-923.",x,x,x,-,x,-,Semi-structured paired debate,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,"10-min English debate on admitting Yemeni refugees, with free turn-taking",10 m 00 s,-,-,-,Tomi,,,
126,1,10.1109/ACII52823.2021.9597451,"Khan, A., Hopkins, J., & Gunes, H. (2021, September). Multi-dimensional Affect in Poetry (POCA) Dataset: Acquisition, Annotation and Baseline Results. In 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-8). IEEE.",-,-,x,-,-,x,Poem recitations,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,,-,-,-,Tomi,,,
127,1,10.1515/cdbme-2023-1139,"Anthiyur Aravindan, A., Kalyan Chappidi, S., Thumma, A., & Palanisamy, R. (2023, September). Prediction of arousal and valence state from electrodermal activity using wavelet based resnet50 model. In Current Directions in Biomedical Engineering (Vol. 9, No. 1, pp. 555-558). De Gruyter.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
128,1,10.1080/09544828.2024.2362589,"Zhang, L., Hu, F., Liu, X., Wang, Y., Zhang, H., Liu, Z., & Yu, C. (2024). Intelligent emotion recognition in product design using multimodal physiological signals and machine learning. Journal of Engineering Design, 1-21.",-,x,-,-,-,x,Product Images,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Participants viewed product images,5 m 15 s,-,-,-,Tomi,,,
129,1,10.1007/s10916-020-01676-6,"Ganapathy, N., Veeranki, Y. R., Kumar, H., & Swaminathan, R. (2021). Emotion recognition using electrodermal activity signals and multiscale deep convolutional neural network. Journal of Medical Systems, 45(4), 49.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
130,1,10.1109/TIM.2024.3420349,"Kumar, P. S., Govarthan, P. K., Gadda, A. A. S., Ganapathy, N., & Ronickom, J. F. A. (2024). Deep learning-based automated emotion recognition using multi modal physiological signals and time-frequency methods. IEEE Transactions on Instrumentation and Measurement.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
130,2,10.1109/TIM.2024.3420349,"Kumar, P. S., Govarthan, P. K., Gadda, A. A. S., Ganapathy, N., & Ronickom, J. F. A. (2024). Deep learning-based automated emotion recognition using multi modal physiological signals and time-frequency methods. IEEE Transactions on Instrumentation and Measurement.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
131,1,10.14236/ewic/HCI2022.19,"Pidgeon, M., Kanwal, N., Murray, N., & Asghar, M. (2022, July). End-to-End Emotion Recognition using Peripheral Physiological Signals. In 35th International BCS Human-Computer Interaction Conference (pp. 1-10). BCS Learning & Development.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
132,1,10.1109/AIHCIR61661.2023.00066,"He, Y., Chen, L., Zhao, Q., Hong, Z., & Chen, Y. (2023, December). MEDA-CBLSTM: Data Acquisition, Processing and Emotion Recognition Based on Multi-Layer EDA. In 2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR) (pp. 380-384). IEEE.",-,x,-,-,x,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,"We use four-minute videos as an emotional trigger script…” and “For the calm state, 20 routine and personal factual articles are prepared for the test subjects to transcribe or recite",-,-,-,-,Tomi,,,
133,1,10.3390/electronics12132795,"Dessai, A., & Virani, H. (2023). Emotion classification based on CWT of ECG and GSR signals using various CNN models. Electronics, 12(13), 2795.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
134,1,10.1109/JBHI.2024.3405491,"Mercado-Diaz, L. R., Veeranki, Y. R., Marmolejo-Ramos, F., & Posada-Quintero, H. F. (2024). EDA-Graph: Graph Signal Processing of Electrodermal Activity for Emotional States Detection. IEEE Journal of Biomedical and Health Informatics.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
135,1,10.1016/j.bspc.2024.106353,"Gahlan, N., & Sethia, D. (2024). AFLEMP: Attention-based federated learning for emotion recognition using multi-modal physiological data. Biomedical Signal Processing and Control, 94, 106353.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
136,1,10.1109/ICCCMLA58983.2023.10346868,"Kumar P, S., & Ronickom, J. F. A. (2023, October). Investigating the Effects of Two-Class Categorical Emotion Classification Through Electrodermal Activity and Machine Learning. In 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA) (pp. 594-599). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
137,1,10.1109/MeMeA54994.2022.9856558,"Zhu, L., Spachos, P., & Gregori, S. (2022, June). Multimodal physiological signals and machine learning for stress detection by wearable devices. In 2022 IEEE International Symposium on Medical Measurements and Applications (MeMeA) (pp. 1-6). IEEE.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
137,2,10.1109/MeMeA54994.2022.9856558,"Zhu, L., Spachos, P., & Gregori, S. (2022, June). Multimodal physiological signals and machine learning for stress detection by wearable devices. In 2022 IEEE International Symposium on Medical Measurements and Applications (MeMeA) (pp. 1-6). IEEE.",x,x,x,-,x,x,Video Stimuli; IAPS; Interactive tests,x,x,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,Math/Logic/Stroop,Five-task protocol combining interactive cognitive tests and emotion-eliciting audio-visual stimuli,-,-,-,-,Tomi,,,
138,1,10.3389/fnins.2022.965871,"Chen, P., Zou, B., Belkacem, A. N., Lyu, X., Zhao, X., Yi, W., ... & Chen, C. (2022). An improved multi-input deep convolutional neural network for automatic emotion recognition. Frontiers in Neuroscience, 16, 965871.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
138,2,10.3389/fnins.2022.965871,"Chen, P., Zou, B., Belkacem, A. N., Lyu, X., Zhao, X., Yi, W., ... & Chen, C. (2022). An improved multi-input deep convolutional neural network for automatic emotion recognition. Frontiers in Neuroscience, 16, 965871.",x,x,x,-,x,x,Video Stimuli; IAPS; Interactive tests,x,x,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,Math/Logic/Stroop,Five-task protocol combining interactive cognitive tests and emotion-eliciting audio-visual stimuli,-,-,-,-,Tomi,,,
138,3,10.3389/fnins.2022.965871,"Chen, P., Zou, B., Belkacem, A. N., Lyu, X., Zhao, X., Yi, W., ... & Chen, C. (2022). An improved multi-input deep convolutional neural network for automatic emotion recognition. Frontiers in Neuroscience, 16, 965871.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 36 affective movie clips,-,51–127s,80,-,Tomi,,,
138,4,10.3389/fnins.2022.965871,"Chen, P., Zou, B., Belkacem, A. N., Lyu, X., Zhao, X., Yi, W., ... & Chen, C. (2022). An improved multi-input deep convolutional neural network for automatic emotion recognition. Frontiers in Neuroscience, 16, 965871.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 20 short emotional film excerpts selected to elicit specific affective states​​,-,34.9-117 s​,81.4 s​​,-,Tomi,,,
139,1,10.1109/AINIT61980.2024.10581751,"Mu, J., Qiao, Y., & Liu, G. (2024, March). Research on Emotion Recognition Strategy Based on Electrocardiogram and Electrodermal activity Signals Induced by Music. In 2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT) (pp. 1574-1578). IEEE.",-,-,x,-,-,x,Music-induced emotion,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,"Participants listened to music stimuli selected from popular music libraries, categorized into fear, happiness, calmness, and sadness. After each piece, they rated valence and arousal on a 9-point scale and selected their felt emotion.","From a pool of 15 pieces of music for each emotion type, the top 10 most fitting pieces, each lasting approximately 18-20 seconds, were chosen as the formal experimental materials.",-,-,-,Tomi,,,
140,1,10.1109/ACII52823.2021.9597434,"Di Lascio, E., Gashi, S., Debus, M. E., & Santini, S. (2021, September). Automatic recognition of flow during work activities using context and physiological signals. In 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-8). IEEE.",-,-,,-,,x,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Natural in-the-wild work tasks,Monitored during their daily work activities. Daily work activities; workers logged each real task ≥ 5 min,44 m 00 s,05 min – 187 min,-,-,Tomi,,,
141,1,10.1007/s12652-021-03462-9,"Ross, K., Hungler, P., & Etemad, A. (2023). Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data. Journal of Ambient Intelligence and Humanized Computing, 14(4), 3199-3224.",-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,,-,-,-,Tomi,,,
142,1,10.3389/fnins.2022.911767,"Perry Fordson, H., Xing, X., Guo, K., & Xu, X. (2022). Emotion recognition with knowledge graph based on electrodermal activity. Frontiers in Neuroscience, 16, 911767.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
142,2,10.3389/fnins.2022.911767,"Perry Fordson, H., Xing, X., Guo, K., & Xu, X. (2022). Emotion recognition with knowledge graph based on electrodermal activity. Frontiers in Neuroscience, 16, 911767.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Observers watched labelled movie clips online,-,0.3 – 5.4 s,-,-,Tomi,,,
143,1,10.1109/JBHI.2022.3224775,"Ghiasi, S., Patane, A., Laurenti, L., Gentili, C., Scilingo, E. P., Greco, A., & Kwiatkowska, M. (2022). Physiologically-informed gaussian processes for interpretable modelling of psycho-physiological states. IEEE Journal of Biomedical and Health Informatics, 27(8), 3721-3730.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
143,2,10.1109/JBHI.2022.3224775,"Ghiasi, S., Patane, A., Laurenti, L., Gentili, C., Scilingo, E. P., Greco, A., & Kwiatkowska, M. (2022). Physiologically-informed gaussian processes for interpretable modelling of psycho-physiological states. IEEE Journal of Biomedical and Health Informatics, 27(8), 3721-3730.",x,x,x,x,-,x,Heat-pain stimulation; Video Stimuli; IAPS,x,x,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,heat pain,PATHWAY thermode on right arm delivered four individually calibrated heat levels; additional IAPS images and validated film clips presented for emotion induction,-,00 m 04 s (max heat plateau) ,-,-,Tomi,,,
143,3,10.1109/JBHI.2022.3224775,"Ghiasi, S., Patane, A., Laurenti, L., Gentili, C., Scilingo, E. P., Greco, A., & Kwiatkowska, M. (2022). Physiologically-informed gaussian processes for interpretable modelling of psycho-physiological states. IEEE Journal of Biomedical and Health Informatics, 27(8), 3721-3730.",-,x,-,-,x,-,Stroop test,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Stroop test,Color-word interference task with 2-s button press responses; buzzer sounds on errors,03 m 00 s (stress phase),-,-,-,Tomi,,,
144,1,10.1109/GLOBECOM48099.2022.10000909,"Zhu, L., & Spachos, P. (2022, December). Annotation efficiency in multimodal emotion recognition with deep learning. In GLOBECOM 2022-2022 IEEE Global Communications Conference (pp. 560-565). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
145,1,10.1109/ICC45041.2023.10278603,"Zhu, L., Spachos, P., & Gregori, S. (2023, May). Electrodermal activity for emotion recognition using cnn and bi-gru model. In ICC 2023-IEEE International Conference on Communications (pp. 5533-5538). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
146,1,10.1007/s11042-022-12711-8,"Katada, S., & Okada, S. (2022). Biosignal-based user-independent recognition of emotion and personality with importance weighting. Multimedia Tools and Applications, 81(21), 30219-30241.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
147,1,10.1109/ICACCS57279.2023.10112973,"Gupta, R., Bhongade, A., & Gandhi, T. K. (2023, March). Multimodal wearable sensors-based stress and affective states prediction model. In 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS) (Vol. 1, pp. 30-35). IEEE.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
148,1,10.1016/j.eswa.2024.124305,"Mohino-Herranz, I., Gil-Pita, R., García-Gómez, J., Alonso-Diaz, S., Rosa-Zurera, M., & Seoane, F. (2024). Initializing the weights of a multilayer perceptron for activity and emotion recognition. Expert Systems with Applications, 253, 124305.",x,x,-,-,-,-,Video Stimuli,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Film clips showing disgust & sadness; neutral baseline,,-,-,-,Tomi,,,
149,1,10.1109/JSEN.2020.3031163,"Woodward, K., & Kanjo, E. (2020). iFidgetCube: tangible fidgeting interfaces (TFIs) to monitor and improve mental wellbeing. IEEE Sensors Journal, 21(13), 14300-14307.",-,-,-,x,x,-,Fidgeting,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Tangible Fidgeting Interface (cube),Participants freely fidgeted with the instrumented cube during daily life while self-labelling mood,,-,-,-,Tomi,,,
150,1,10.1145/3490686,"Yin, G., Sun, S., Yu, D., Li, D., & Zhang, K. (2022). A multimodal framework for large-scale emotion recognition by fusing music and electrodermal activity signals. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(3), 1-23.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,-,,,Tomi
150,2,10.1145/3490686,"Yin, G., Sun, S., Yu, D., Li, D., & Zhang, K. (2022). A multimodal framework for large-scale emotion recognition by fusing music and electrodermal activity signals. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(3), 1-23.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
150,3,10.1145/3490686,"Yin, G., Sun, S., Yu, D., Li, D., & Zhang, K. (2022). A multimodal framework for large-scale emotion recognition by fusing music and electrodermal activity signals. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(3), 1-23.",-,-,x,-,-,x,Music-listening,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants listened to 20 chorus excerpts of pop songs and provided continuous valence/arousal ratings​​,-,-,-,-,Tomi,,,
151,1,10.3390/s23031608,"Baldassarri, S., García de Quirós, J., Beltrán, J. R., & Álvarez, P. (2023). Wearables and machine learning for improving runners’ motivation from an affective perspective. Sensors, 23(3), 1608.",-,-,x,-,-,x,Music listening playlist,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,Nine songs (two per Russell quadrant) were played with 15-s pauses,The total duration of the playlist is approximately 41 minutes.,-,41 m 00 s,-,Tomi,,,
152,1,10.3389/fnins.2023.1138091,"Li, Z., Xing, Y., Pi, Y., Jiang, M., & Zhang, L. (2023). A novel physiological feature selection method for emotional stress assessment based on emotional state transition. Frontiers in Neuroscience, 17, 1138091.",-,x,-,-,-,x,IAPS,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,"120 IAPS images (3 × 40) shown for 6 s each to evoke negative, neutral, and positive emotions",Each picture was presented for 6 s.,-,12 m 00 s,-,Tomi,,,
153,1,10.3390/s21113760,"Alanazi, S.A.; Alruwaili, M.; Ahmad, F.; Alaerjan, A.; Alshammari, N. Estimation of Organizational Competitiveness by a Hybrid of One-Dimensional Convolutional Neural Networks and Self-Organizing Maps Using Physiological Signals for Emotional Analysis of Employees. Sensors 2021, 21, 3760.",-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,,-,-,-,Tomi,,,
154,1,10.3390/electronics13081494,"Joo, J. H., Han, S. H., Park, I., & Chung, T. S. (2024). Immersive Emotion Analysis in VR Environments: A Sensor-Based Approach to Prevent Distortion. Electronics, 13(8), 1494.",x,x,x,-,-,x,360° Video-Based Virtual Environments,-,x,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,Immersive 360° videos delivered in VR covering four CMA emotion quadrants ​,-,01 m 00 s – 03 m 00 s,,-,Tomi,,,
155,1,10.3390/s21062166,"Oh, G., Ryu, J., Jeong, E., Yang, J. H., Hwang, S., Lee, S., & Lim, S. (2021). Drer: Deep learning–based driver’s real emotion recognizer. Sensors, 21(6), 2166.",x,x,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,,17 m 00 s –19 m 00 s,-,-,Tomi,,,
156,1,10.1016/j.inffus.2020.08.007,"Raheel, A., Majid, M., & Anwar, S. M. (2021). DEAR-MULSEMEDIA: Dataset for emotion analysis and recognition in response to multiple sensorial media. Information Fusion, 65, 37-49.",x,x,x,x,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,-,Mulsemedia content,-,02 m 37 s,00 m 21 s – 00 m 58 s,00 m 39.25 s,-,Tomi,,,
157,1,10.1109/MeMeA60663.2024.10596800,"Veeranki, Y. R., Mercado-Diaz, L. R., & Posada-Quintero, H. F. (2024, June). Autoencoder Based Nonlinear Feature Extraction from EDA Signals for Emotion Recognition. In 2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA) (pp. 1-5). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
158,1,10.1109/PerComWorkshops59983.2024.10502631,"Jaiswal, D., Mukhopadhyay, S., & Sharma, V. (2024, March). Tinystressnet: On-device stress assessment with wearable sensors on edge devices. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 166-171). IEEE.",-,-,-,-,x,-,Real-world driving,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,"31 km real-world route including urban, highway and rest segments",≈ 1 h 26 m (per drive),-,-,-,Tomi,,,
158,2,10.1109/PerComWorkshops59983.2024.10502631,"Jaiswal, D., Mukhopadhyay, S., & Sharma, V. (2024, March). Tinystressnet: On-device stress assessment with wearable sensors on edge devices. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 166-171). IEEE.",-,x,-,-,x,-,Mental arithmetic,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,math,Timed on-screen arithmetic equations answered aloud under evaluation pressure,05 m 00 s,-,5 min,-,Tomi,,,
158,3,10.1109/PerComWorkshops59983.2024.10502631,"Jaiswal, D., Mukhopadhyay, S., & Sharma, V. (2024, March). Tinystressnet: On-device stress assessment with wearable sensors on edge devices. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 166-171). IEEE.",x,x,x,-,x,-,TSST,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,speech/math,"Semi-automated digital TSST with 5-min preparation, 3-min extempore speech before a video panel, followed by a 5-min mental-arithmetic task",3 min speech / 5 min aritmetic task,-,-,-,Tomi,,,
158,4,10.1109/PerComWorkshops59983.2024.10502631,"Jaiswal, D., Mukhopadhyay, S., & Sharma, V. (2024, March). Tinystressnet: On-device stress assessment with wearable sensors on edge devices. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 166-171). IEEE.",x,x,x,-,x,,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
159,1,10.2478/jaiscr-2021-0001,"Rahman, J. S., Gedeon, T., Caldwell, S., Jones, R., & Jin, Z. (2021). Towards effective music therapy for mental health care using machine learning tools: human affective reasoning and music genres. Journal of Artificial Intelligence and Soft Computing Research, 11(1), 5-20.",-,-,x,-,-,x,Music listening,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,-,,-,-,-,Tomi,,,
160,1,10.1109/TAFFC.2021.3056960,"Sabour, R. M., Benezeth, Y., De Oliveira, P., Chappe, J., & Yang, F. (2021). Ubfc-phys: A multimodal database for psychophysiological studies of social stress. IEEE Transactions on Affective Computing, 14(1), 622-636.",x,x,x,-,x,,Modified Trier Social Stress Test (TSST),-,-,-,-,,-,-,-,-,x,-,-,-,-,-,-,-,,"10-min rest baseline, 6-min job-interview speech, 4-min mental arithmetic (count-down) in evaluative setting.",20 m 00 s,-,-,-,Tomi,,,
161,1,10.1109/TAFFC.2019.2901673,"Shukla, J., Barreda-Ángeles, M., Oliver, J., Puig, D., & Nandi, G. C. (2019). Feature Extraction and Selection for Emotion Recognition from Electrodermal Activity. IEEE Transactions on Affective Computing, 12(4), 858-866.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
162,1,10.1109/TENSYMP54529.2022.9864492,"Chatterjee, D., Gavas, R., & Saha, S. K. (2022, July). Exploring skin conductance features for cross-subject emotion recognition. In 2022 IEEE Region 10 Symposium (TENSYMP) (pp. 1-6). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 36 affective movie clips,-,51–127s,80,-,Tomi,,,
163,1,10.1145/3495002,"Tabbaa, L., Searle, R., Bafti, S. M., Hossain, M. M., Intarasisrisawat, J., Glancy, M., & Ang, C. S. (2021). Vreed: Virtual reality emotion recognition dataset using eye tracking & physiological measures. Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies, 5(4), 1-20.",x,x,x,-,-,x,Virtual Reality,-,x,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,,Participants engaged in twelve 360-degree virtual environments (360-VEs) designed to elicit specific emotional states.,,,,,Tomi,,,
164,1,10.1049/ccs2.12107,"Zou, C., Deng, Z., He, B., Yan, M., Wu, J., & Zhu, Z. (2024). Emotion classification with multi‐modal physiological signals using multi‐attention‐based neural network. Cognitive Computation and Systems, 6(1-3), 1-11.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
165,1,10.2478/joeb-2021-0021,"Jacobsen, F. A., Hafli, E. W., Tronstad, C., & Martinsen, Ø. G. (2021). Classification of emotions based on electrodermal activity and transfer learning-a pilot study. Journal of Electrical Bioimpedance, 12(1), 178.",x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,,-,,-,-,-,Tomi,,,
166,1,10.3389/fcomp.2023.1264713,"Gohumpu, J., Xue, M., & Bao, Y. (2023). Emotion recognition with multi-modal peripheral physiological signals. Frontiers in Computer Science, 5, 1264713.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
167,1,10.1109/JBHI.2023.3239305,"Zhu, L., Spachos, P., Ng, P. C., Yu, Y., Wang, Y., Plataniotis, K., & Hatzinakos, D. (2023). Stress detection through wrist-based electrodermal activity monitoring and machine learning. IEEE Journal of Biomedical and Health Informatics, 27(5), 2155-2165.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
167,2,10.1109/JBHI.2023.3239305,"Zhu, L., Spachos, P., Ng, P. C., Yu, Y., Wang, Y., Plataniotis, K., & Hatzinakos, D. (2023). Stress detection through wrist-based electrodermal activity monitoring and machine learning. IEEE Journal of Biomedical and Health Informatics, 27(5), 2155-2165.",x,x,x,-,x,x,Video Stimuli; IAPS; Interactive tests,x,x,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,Math/Logic/Stroop,Five-task protocol combining interactive cognitive tests and emotion-eliciting audio-visual stimuli,-,-,-,-,Tomi,,,
167,3,10.1109/JBHI.2023.3239305,"Zhu, L., Spachos, P., Ng, P. C., Yu, Y., Wang, Y., Plataniotis, K., & Hatzinakos, D. (2023). Stress detection through wrist-based electrodermal activity monitoring and machine learning. IEEE Journal of Biomedical and Health Informatics, 27(5), 2155-2165.",x,x,x,-,x,x,Mental arithmetic; Stroop; Running; Video Stimuli; Relaxing music,-,x,-,-,x,-,x,-,-,-,-,-,-,-,-,-,-,math/running,-,5 min each activity,-,-,-,Tomi,,,
167,4,10.1109/JBHI.2023.3239305,"Zhu, L., Spachos, P., Ng, P. C., Yu, Y., Wang, Y., Plataniotis, K., & Hatzinakos, D. (2023). Stress detection through wrist-based electrodermal activity monitoring and machine learning. IEEE Journal of Biomedical and Health Informatics, 27(5), 2155-2165.",x,x,x,-,x,-,Public-speaking (real & VR) presentations,-,-,-,-,-,-,x,-,-,x,x,-,-,-,-,-,-,-,5-min prepared talks delivered to real or virtual audiences,05 m 00 s (nominal),-,-,-,Tomi,,,
168,1,10.1142/S0219477522500134,"Veeranki, Y. R., Ganapathy, N., & Swaminathan, R. (2022). Analysis of fluctuation patterns in emotional states using electrodermal activity signals and improved symbolic aggregate approximation. Fluctuation and Noise Letters, 21(02), 2250013.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,60s,-,-,-,Tomi,,,
169,1,10.1016/j.bspc.2024.106224,"Umair, M., Rashid, N., Khan, U. S., Hamza, A., & Iqbal, J. (2024). Emotion fusion-sense (Emo Fu-sense)–a novel multimodal emotion classification technique. Biomedical Signal Processing and Control, 94, 106224.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 20 short emotional film excerpts selected to elicit specific affective states​​,-,34.9-117 s​,81.4 s​​,-,Tomi,,,
170,1,10.1109/TAFFC.2018.2884461,"Miranda-Correa, J. A., Khomami Abadi, M., Sebe, N., & Patras, I. (2017). AMIGOS: A dataset for affect, personality and mood research on individuals and groups. IEEE Transactions on Affective Computing.",x,x,x,-,-,-,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Short emotional videos / long videos,,-,-,-,Tomi,,,
171,1,10.1109/BIBM58861.2023.10385273,"Singh, A., Wittenberg, T., Salman, M. M., Holzer, N., Göb, S., Pahl, J., ... & Sawant, S. (2023, December). Bio-Signal Based Multimodal Fusion with Bilinear Model for Emotion Recognition. In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) (pp. 4834-4839). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 20 short emotional film excerpts selected to elicit specific affective states​​,-,34.9-117 s​,81.4 s​​,-,Tomi,,,
172,1,10.1109/BSN63547.2024.10780682,"Gahlan, N., Sethia, D., & Ray, S. B. (2024, October). Emotion Analysis Using Auditory ASMR via Physiological Signals and Federated Learning. In 2024 IEEE 20th International Conference on Body Sensor Networks (BSN) (pp. 1-4). IEEE.",-,-,x,-,-,x,ASMR audio stimuli,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,“Blossoming playful upbeat music”; “Crushing Crunchy & Soft Things” (ASMR sounds),Participants “sat straight and meditated … then they were asked to listen to two ASMR audios”,05 m 00 s,,-,-,Tomi,,,
173,1,10.1109/TIM.2024.3500058,"Kumar P, S., & Fredo Agastinose Ronickom, J. (2025). Emotion Classification Through Optimal Segments of EDA and Texture Analysis of Time-Encoded Images With Artificial Intelligence. IEEE Transactions on Instrumentation Measurement, 74, 3500058.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"Participants watched validated emotional video-stimuli (e.g., amusing, boring, relaxing, scary) while annotating their emotional experience using a joystick interface.",-,119 s - 197 s,-,-,Tomi,,,
173,2,10.1109/TIM.2024.3500058,"Kumar P, S., & Fredo Agastinose Ronickom, J. (2025). Emotion Classification Through Optimal Segments of EDA and Texture Analysis of Time-Encoded Images With Artificial Intelligence. IEEE Transactions on Instrumentation Measurement, 74, 3500058.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
174,1,10.1145/3581783.3612277,"Liu, Y., Jia, Z., & Wang, H. (2023, October). Emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals. In Proceedings of the 31st ACM International Conference on Multimedia (pp. 6122-6131).",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
174,2,10.1145/3581783.3612277,"Liu, Y., Jia, Z., & Wang, H. (2023, October). Emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals. In Proceedings of the 31st ACM International Conference on Multimedia (pp. 6122-6131).",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 20 short emotional film excerpts selected to elicit specific affective states​​,-,34.9-117 s​,81.4 s​​,-,Tomi,,,
175,1,10.3390/electronics13153019,"Feng, G., Wang, H., Wang, M., Zheng, X., & Zhang, R. (2024). A Research on Emotion Recognition of the Elderly Based on Transformer and Physiological Signals. Electronics, 13(15), 3019.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,Video-evoked method… six suitable video segments were selected,,-,-,-,Tomi,,,
176,1,10.1109/TAFFC.2023.3315973,"Shui, X., Lin, R., Luo, Z., Lin, B., Mao, X., Li, H., ... & Zhang, D. (2023). Bodily electrodermal representations for affective computing. IEEE Transactions on Affective Computing, 15(3), 1018-1025.",-,x,-,-,-,x,IAPS,x,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,Each video clip lasted approximately 1 minute and 60 seconds.,-,12 m 00 s,-,Tomi,,,
177,1,10.1109/HRI53351.2022.9889545,"Mohamed, Y., Ballardini, G., Parreira, M. T., Lemaignan, S., & Leite, I. (2022, March). Automatic frustration detection using thermal imaging. In 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI) (pp. 451-459). IEEE.",x,x,x,-,x,,Multple techniques,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Failure-induced quiz & dual-task cognitive-load game with NAO robot,Participants watched a series of emotional video clips designed to elicit specific emotional states.,13 m 00 s,-,-,-,Tomi,,,
178,1,10.3389/fpsyg.2023.1293513,"Başaran, O. T., Can, Y. S., André, E., & Ersoy, C. (2024). Relieving the burden of intensive labeling for stress monitoring in the wild by using semi-supervised learning. Frontiers in Psychology, 14, 1293513.",-,-,-,-,x,-,TSST,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,speech/math,5‑min speech about qualifications + 5‑min serial subtraction in front of neutral evaluators,10 m 00 s,-,-,-,Tomi,,,
179,1,10.1145/3544793.3563427,"Alchieri, L., Abdalazim, N., Alecci, L., Gashi, S., Di Lascio, E., & Santini, S. (2022, September). On the impact of lateralization in physiological signals from wearable sensors. In Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers (pp. 472-477).",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,10‑minute compilation of 10 funny YouTube clips designed to elicit natural laughter,-,7–151 s,69 s,-,Tomi,,,
180,1,10.3390/s23020963,"Stržinar, Ž., Sanchis, A., Ledezma, A., Sipele, O., Pregelj, B., & Škrjanc, I. (2023). Stress detection using frequency spectrum analysis of wrist-measured electrodermal activity. Sensors, 23(2), 963.",x,x,x,-,x,x,TSST; Video Stimuli; guided meditation,-,x,-,-,-,-,x,-,-,x,-,x,-,-,-,-,-,speech/math,Lab protocol: neutral baseline → amusing video clips → Trier Social Stress Test (speech + mental arithmetic) → guided breathing meditation,392 s (video) / ≈ 600 s (TSST),392 s – 10 min,-,-,Tomi,,,
181,1,10.1016/j.bspc.2021.102756,"Acevedo, C. M. D., Gómez, J. K. C., & Rojas, C. A. A. (2021). Academic stress detection on university students during COVID-19 outbreak by using an electronic nose and the galvanic skin response. Biomedical Signal Processing and Control, 68, 102756.",,,,,x,,Virtual-exam academic-stress protocol,,,,,,,,,,,,,,,,,,Academic task,Samples were collected while the students were presenting a virtual exam; relaxation samples were gathered once the students finished the exam,05 m 00 s,,,,Tomi,,,
182,1,10.1109/ACII52823.2021.9597442,"Elalamy, R., Fanourakis, M., & Chanel, G. (2021, September). Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals. In 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants passively watched 40 one-minute music-video excerpts intended to elicit emotions,01 m 00 s,-,-,-,Tomi,,,
182,2,10.1109/ACII52823.2021.9597442,"Elalamy, R., Fanourakis, M., & Chanel, G. (2021, September). Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals. In 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,x,x,-,-,x,Video Stimuli,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,Participants watched 16 short clips (51–150 s) and 4 long excerpts (14–24 min) selected to cover the four quadrants of the valence–arousal space ​,-,00 m 51 s – 02 m 30 s,01 m 27 s (μ = 86.7 s),-,Tomi,,,
